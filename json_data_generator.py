# json_data_generator.py
# ä¸ºutoolsåº”ç”¨ç”ŸæˆJSONæ ¼å¼çš„æ•°æ®

import json
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any
from collections import defaultdict, Counter

class JSONDataGenerator:
    """ä¸ºutoolsåº”ç”¨ç”ŸæˆJSONæ ¼å¼çš„è´¢è”ç¤¾æ•°æ®"""
    
    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.json_output_dir = self.output_dir / "json"
        self.json_output_dir.mkdir(parents=True, exist_ok=True)
        
        # å…³é”®è¯é…ç½®
        self.keywords = ["æœºå™¨äºº", "æ— äººæœº", "å†›å·¥", "ç®—åŠ›", "èŠ¯ç‰‡", "æ–°èƒ½æº", "äººå·¥æ™ºèƒ½", "åŠå¯¼ä½“"]
        self.stock_list = ["åä¸º", "è…¾è®¯", "è¯æ˜åº·å¾·", "ç†æƒ³æ±½è½¦", "äº¬ä¸œ", "å®å¾·æ—¶ä»£", "å±±æ²³æ™ºèƒ½", 
                          "å°ç±³", "é•¿åŸå†›å·¥", "èƒœé€šèƒ½æº", "ä¸œæ°æ™ºèƒ½", "ä¸­é™…æ—­åˆ›", "ä¸­é©¬ä¼ åŠ¨", 
                          "ä¸Šçº¬æ–°æ", "é˜¿é‡Œå·´å·´", "ç¦å…ƒåŒ»è¯", "åˆ©å¾·æ›¼", "èˆªå¤©ç”µå­", "æ€ç‰¹å¥‡", 
                          "ç½‘æ˜“", "æ¯”äºšè¿ª", "TCL", "è‹±ä¼Ÿè¾¾", "è°·æ­Œ"]
    
    def generate_all_json_data(self) -> None:
        """ç”Ÿæˆæ‰€æœ‰JSONæ•°æ®æ–‡ä»¶"""
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] å¼€å§‹ç”ŸæˆJSONæ•°æ®...")
        
        # è·å–æœ€è¿‘5å¤©çš„æ•°æ®
        news_data = self._extract_news_data()
        hotspot_data = self._generate_hotspot_data(news_data)
        stock_data = self._generate_stock_data(news_data)
        
        # ç”Ÿæˆä¸»æ•°æ®æ–‡ä»¶
        main_data = {
            "lastUpdate": datetime.now().isoformat(),
            "newsData": news_data,
            "hotspotData": hotspot_data,
            "stockData": stock_data,
            "dateRange": self._get_date_range()
        }
        
        # ä¿å­˜ä¸»æ•°æ®æ–‡ä»¶
        main_file = self.json_output_dir / "cailianpress_data.json"
        try:
            with open(main_file, 'w', encoding='utf-8') as f:
                json.dump(main_data, f, ensure_ascii=False, indent=2)
            
            # éªŒè¯ç”Ÿæˆçš„JSONæ–‡ä»¶
            with open(main_file, 'r', encoding='utf-8') as f:
                json.load(f)  # å°è¯•è§£æä»¥éªŒè¯JSONæœ‰æ•ˆæ€§
            print(f"ä¸»æ•°æ®æ–‡ä»¶éªŒè¯æˆåŠŸ: {main_file}")
        except Exception as e:
            print(f"ä¸»æ•°æ®æ–‡ä»¶ç”Ÿæˆæˆ–éªŒè¯å¤±è´¥: {e}")
            raise
        
        # ç”Ÿæˆç®€åŒ–ç‰ˆæœ¬ï¼ˆç”¨äºå¿«é€ŸåŠ è½½ï¼‰
        summary_data = {
            "lastUpdate": main_data["lastUpdate"],
            "newsCount": len(news_data),
            "dateRange": main_data["dateRange"],
            "topKeywords": hotspot_data[:10],  # åªä¿ç•™å‰10ä¸ªçƒ­ç‚¹
            "topStocks": stock_data[:15]       # åªä¿ç•™å‰15ä¸ªè‚¡ç¥¨
        }
        
        summary_file = self.json_output_dir / "cailianpress_summary.json"
        try:
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=2)
            
            # éªŒè¯ç”Ÿæˆçš„JSONæ–‡ä»¶
            with open(summary_file, 'r', encoding='utf-8') as f:
                json.load(f)  # å°è¯•è§£æä»¥éªŒè¯JSONæœ‰æ•ˆæ€§
            print(f"æ‘˜è¦æ•°æ®æ–‡ä»¶éªŒè¯æˆåŠŸ: {summary_file}")
        except Exception as e:
            print(f"æ‘˜è¦æ•°æ®æ–‡ä»¶ç”Ÿæˆæˆ–éªŒè¯å¤±è´¥: {e}")
            raise
        
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] JSONæ•°æ®ç”Ÿæˆå®Œæˆ")
        print(f"  - æ–°é—»æ•°æ®: {len(news_data)} æ¡")
        print(f"  - çƒ­ç‚¹æ•°æ®: {len(hotspot_data)} ä¸ª")
        print(f"  - è‚¡ç¥¨æ•°æ®: {len(stock_data)} ä¸ª")
    
    def _extract_news_data(self) -> List[Dict[str, Any]]:
        """ä»markdownæ–‡ä»¶ä¸­æå–æ–°é—»æ•°æ®"""
        news_data = []
        current_time = datetime.now()
        
        # è·å–æœ€è¿‘5å¤©çš„æ–‡ä»¶
        for day_offset in range(5):
            target_date = current_time - timedelta(days=day_offset)
            date_str = target_date.strftime("%Y-%m-%d")
            file_path = self.output_dir / f"cls_{date_str}.md"
            
            if not file_path.exists():
                continue
            
            try:
                content = file_path.read_text(encoding="utf-8")
                daily_news = self._parse_markdown_content(content, date_str)
                news_data.extend(daily_news)
            except Exception as e:
                print(f"è§£ææ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        
        # æŒ‰æ—¶é—´å€’åºæ’åˆ—
        news_data.sort(key=lambda x: f"{x['date']} {x['time']}", reverse=True)
        return news_data
    
    def _parse_markdown_content(self, content: str, date_str: str) -> List[Dict[str, Any]]:
        """è§£æmarkdownå†…å®¹ï¼Œæå–æ–°é—»æ¡ç›®"""
        news_items = []
        lines = content.split('\n')
        current_type = "general"
        
        for line in lines:
            line = line.strip()
            
            # è¯†åˆ«ç« èŠ‚ç±»å‹
            if "ğŸ”´ é‡è¦ç”µæŠ¥" in line:
                current_type = "important"
                continue
            elif "ğŸ“° ä¸€èˆ¬ç”µæŠ¥" in line:
                current_type = "general"
                continue
            
            # è§£ææ–°é—»æ¡ç›®
            if line.startswith("- [") and "]" in line:
                news_item = self._parse_news_line(line, date_str, current_type)
                if news_item:
                    news_items.append(news_item)
        
        return news_items
    
    def _parse_news_line(self, line: str, date_str: str, news_type: str) -> Dict[str, Any]:
        """è§£æå•æ¡æ–°é—»è¡Œ"""
        try:
            # æå–æ—¶é—´
            time_match = re.search(r'\[(\d{2}:\d{2})\]', line)
            time_str = time_match.group(1) if time_match else "00:00"
            
            # æå–å†…å®¹å’Œé“¾æ¥
            content_match = re.search(r'\] (.+?)(?:\(https://www\.cls\.cn/detail/(\d+)\))?$', line)
            if not content_match:
                return None
            
            content = content_match.group(1)
            # ç§»é™¤markdownæ ¼å¼
            content = re.sub(r'\*\*(.+?)\*\*', r'\1', content)  # ç§»é™¤ç²—ä½“
            content = re.sub(r'\[(.+?)\]\(.+?\)', r'\1', content)  # ç§»é™¤é“¾æ¥æ ¼å¼ï¼Œä¿ç•™æ–‡æœ¬
            
            # æ¸…ç†å¯èƒ½å¯¼è‡´JSONé”™è¯¯çš„å­—ç¬¦
            content = content.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
            content = re.sub(r'\s+', ' ', content)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
            content = content.strip()
            
            # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
            if not content:
                return None
            
            return {
                "date": date_str,
                "time": time_str,
                "type": news_type,
                "content": content
            }
        except Exception as e:
            print(f"è§£ææ–°é—»è¡Œå¤±è´¥: {line}, é”™è¯¯: {e}")
            return None
    
    def _generate_hotspot_data(self, news_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆçƒ­ç‚¹æ•°æ®"""
        # æŒ‰æ—¥æœŸå’Œå…³é”®è¯ç»Ÿè®¡
        daily_keyword_stats = defaultdict(lambda: defaultdict(lambda: {"positive": 0, "negative": 0, "neutral": 0}))
        
        for news in news_data:
            date = news.get("date", "")
            content = news.get("content", "")
            news_type = news.get("type", "general")
            
            # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
            if not date or not content or not isinstance(content, str):
                continue
                
            for keyword in self.keywords:
                if keyword in content:
                    # ç®€å•çš„æƒ…æ„Ÿåˆ†æ
                    if news_type == "important":
                        if any(word in content for word in ["åˆ©å¥½", "ä¸Šæ¶¨", "çªç ´", "å¢é•¿", "æ¶¨åœ"]):
                            daily_keyword_stats[date][keyword]["positive"] += 1
                        elif any(word in content for word in ["åˆ©ç©º", "ä¸‹è·Œ", "æš´è·Œ", "é£é™©", "è­¦å‘Š"]):
                            daily_keyword_stats[date][keyword]["negative"] += 1
                        else:
                            daily_keyword_stats[date][keyword]["neutral"] += 1
                    else:
                        daily_keyword_stats[date][keyword]["neutral"] += 1
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼Œç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        hotspot_data = []
        for date, keywords in daily_keyword_stats.items():
            if not isinstance(date, str) or not date.strip():
                continue
            for keyword, stats in keywords.items():
                if not isinstance(keyword, str) or not keyword.strip():
                    continue
                hotspot_data.append({
                    "date": date.strip(),
                    "keyword": keyword.strip(),
                    "positive": int(stats["positive"]),
                    "negative": int(stats["negative"]),
                    "neutral": int(stats["neutral"])
                })
        
        return hotspot_data
    
    def _generate_stock_data(self, news_data: List[Dict[str, Any]]) -> List[List]:
        """ç”Ÿæˆè‚¡ç¥¨æåŠæ•°æ®"""
        stock_counts = Counter()
        
        for news in news_data:
            content = news.get("content", "")
            if not content or not isinstance(content, str):
                continue
                
            for stock in self.stock_list:
                if isinstance(stock, str) and stock.strip() and stock in content:
                    stock_counts[stock] += 1
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼ŒæŒ‰æåŠæ¬¡æ•°æ’åºï¼Œç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        stock_data = []
        for stock, count in stock_counts.most_common():
            if isinstance(stock, str) and stock.strip() and isinstance(count, int):
                stock_data.append([stock.strip(), count])
        
        return stock_data
    
    def _get_date_range(self) -> Dict[str, str]:
        """è·å–æ•°æ®æ—¥æœŸèŒƒå›´"""
        current_time = datetime.now()
        start_date = current_time - timedelta(days=4)
        return {
            "start": start_date.strftime("%Y-%m-%d"),
            "end": current_time.strftime("%Y-%m-%d")
        }

if __name__ == "__main__":
    generator = JSONDataGenerator("./output/cls")
    generator.generate_all_json_data()
